{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "import sklearn.cross_validation as cv\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.naive_bayes import MultinomialNB \n",
      "import pandas as pd\n",
      "import numpy as np\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ...continued from data_prep.ipynb\n",
      "\n",
      "# 3a) What did you learn from the initial exploration phase?\n",
      "\n",
      "# To create an uplifting/joyful set of articles, do I:\n",
      "# 1) cherry-pick joyful articles?\n",
      "# 2) filter out the negative ones?\n",
      "\n",
      "# There are clear (universal, unambiguous) positives:\n",
      "# \"Freed Captive Is Reunited With His Family in Britain\"\n",
      "\n",
      "# Clear negatives:\n",
      "# \"Brazil: Deadly Mudslide at Resort Near Rio\"\n",
      "\n",
      "# And many, many ambiguous articles:\n",
      "# C.I.A. Takes On Expanded Role On Front Lines\n",
      "# Rush Limbaugh Released From Hospital\n",
      "\n",
      "# I decided to score an article as \"joyful\" only if it made me joyful.\n",
      "# That way, I maintained a clear signal inspite of the noise from negative\n",
      "# and ambiguous articles. I also accepted that controlling for individual bias\n",
      "# was outside the scope of this project. All articles were scored with my personal bias.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 308
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Note, as mentioned in my presentation and later in this notebook, I decided to add additional\n",
      "# joyful-scored articles to my corpus. The file below is the final training data set.\n",
      "\n",
      "articles = pd.read_csv('2010_ny_added_positives.csv')\n",
      "\n",
      "# articles = articles[:187]\n",
      "# I tried training models on subsets of my master set of ~769 articles. Predictably, the models \n",
      "# degraded roughly linearly with the reduction of the number of training documents. \n",
      "# I could have vastly improved my models by adding additional training data, but I did not have\n",
      "# time to score thousands or millions of articles."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles.groupby(articles.uplifting).count()\n",
      "\n",
      "# uplifting_prior_prob = 94 / 769\n",
      "# base_prior_prob = 675 / 769\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "//anaconda/lib/python2.7/site-packages/pandas/core/config.py:570: DeprecationWarning: height has been deprecated.\n",
        "\n",
        "  warnings.warn(d.msg, DeprecationWarning)\n",
        "//anaconda/lib/python2.7/site-packages/pandas/core/config.py:570: DeprecationWarning: height has been deprecated.\n",
        "\n",
        "  warnings.warn(d.msg, DeprecationWarning)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Unnamed: 0</th>\n",
        "      <th>lead_paragraph</th>\n",
        "      <th>headline</th>\n",
        "      <th>uplifting</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>uplifting</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 675</td>\n",
        "      <td> 675</td>\n",
        "      <td> 675</td>\n",
        "      <td> 675</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>  94</td>\n",
        "      <td>  94</td>\n",
        "      <td>  94</td>\n",
        "      <td>  94</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "           Unnamed: 0  lead_paragraph  headline  uplifting\n",
        "uplifting                                                 \n",
        "0                 675             675       675        675\n",
        "1                  94              94        94         94"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I tried training the data set on the 'headline' text only, 'lead_paragraph' only, and 'headline'\n",
      "# + 'lead paragraph.' The two merged features did not perform significantly better than \n",
      "# 'lead_paragraph' alone, so I used that feature set only from then on:\n",
      "\n",
      "# Naive Bayes model, GAUSS, 3-fold cross validation:\n",
      "# (lead_paragraph only)\n",
      "# 0.908       0.89558233  0.89156627]\n",
      "\n",
      "# (headline only)\n",
      "# [ 0.732       0.78714859  0.74297189]\n",
      "\n",
      "# [headline + paragraph]\n",
      "#  0.908       0.89156627  0.8875502 ]\n",
      "\n",
      "# Code commented out, as it was abandoned:\n",
      "\n",
      "#MERGE lead_paragraph and headline data into a single column (no significant improvement)\n",
      "# join_col = pd.Series([])\n",
      "# for i in range(len(articles.lead_paragraph)):\n",
      "#     join_col[i] = articles.lead_paragraph[i] + \" \" + articles.headline[i]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 351
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 4) Describe what kinds of statistical methods you used, and perhaps others you considered but\n",
      "# did not use, and how you decided what to use.\n",
      "\n",
      "# For all models, I first prepared the data by converting each bag of words into\n",
      "# a TFIDF vector. Justifications:\n",
      "# a) TFIDF vectorization is space efficient, because it handles sparse data sets efficiently.\n",
      "# That is, it does not need to store data on the global set of words for every document,\n",
      "# but rather only the words that actually appear in that document.\n",
      "# b) TF IDF is well respected in the information retrieval community for accurately modeling\n",
      "# documents as bags of words. For example, it is used in the Lucene Project to calculate \n",
      "# document similarity: https://lucene.apache.org/.\n",
      "# c) TFIDF normalizes a term's frequency with respect to its inter-document scarcity. This\n",
      "# prevents a model overfitting highly-used, but low-value words.\n",
      "\n",
      "#VECTORIZE\n",
      "vectorizer = TfidfVectorizer( stop_words='english')\n",
      "X_train = vectorizer.fit_transform(articles.lead_paragraph)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#MODELS\n",
      "\n",
      "#Note: uncomment a model to run for the rest of the notebook.\n",
      "\n",
      "# model = RandomForestClassifier(n_estimators=30)\n",
      "\n",
      "# RandomForest yielded an error of .87, with a false positive rate of 2/94 = 2.1%\n",
      "# Thus, it was the most accurate model. However, it required 30 estimators to attain this\n",
      "# level of accuracy, so I don't plan to use it in a production implementation that needs to be\n",
      "# updated in anything close to real time.\n",
      "\n",
      "# I started with Random Forest both as a final model but also as an exploratory step. I \n",
      "# hoped to find that certain words were highly indicative of joyfulness in a document. \n",
      "# Unfortunately, I found that different training runs yielded wildly different \n",
      "# \"important features.\" I concluded that my document set was too sparse, and my documents\n",
      "# too short, to reliably find words that predict joy in isolation. An example list\n",
      "# is available  a few paragraphs below. Two additional example lists are available in \n",
      "# prezi.pdf, slide 14.\n",
      "\n",
      "\n",
      "# model = GaussianNB( ) # assumes normal distribution of terms\n",
      "\n",
      "# GaussianNB yielded an error of .870, with a false positive rate of 4/94 = 4.6%\n",
      "\n",
      "# model = MultinomialNB(alpha=.1) \n",
      "\n",
      "# MultinomialNB's false positive rate approached 100% if alpha was higher or lower than .1 (\n",
      "# I discovered this by trial and error. Given that even with the correct alpha it did not \n",
      "# outperform GaussianNB, I discarded it.\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 353
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TRAIN\n",
      "trained_model = model.fit(X_train.toarray(), articles.uplifting)\n",
      "cvs = cross_val_score(trained_model, X_train.toarray(), articles.uplifting)\n",
      "\n",
      "# Prints the mean error for the model\n",
      "print np.array(cvs).sum() / 3\n",
      "\n",
      "\n",
      "y_pred = trained_model.predict(X_train.toarray())\n",
      "print(\"Number of mislabeled points : %d\" % (articles.uplifting != y_pred).sum())\n",
      "print(\"Number of sad articles marked uplifting: : %d\" % (articles.uplifting < y_pred).sum())\n",
      "print(\"Number of uplifting articles marked sad: : %d\" % (articles.uplifting > y_pred).sum())\n",
      "\n",
      "# False positives are much more damaging to this project, since 1 depressing document can ruin\n",
      "# the experience of reading 9 proceeding uplifting documents. However, falsely marking an\n",
      "# uplifting document as depressing will not noticeably affect the reading experience, as\n",
      "# other uplifting documents may be substituted.\n",
      "\n",
      "# Fortunately, I decided to break the model error into false positives and false negatives.\n",
      "# I found that, given that my data was skewed heavily towards non-joyful documents, my models\n",
      "# could yield an error of ~.90 while scoring every single joyful document incorrectly!\n",
      "# I was able to mitigate this by adding more joyful documents to the training set. This\n",
      "# drastically improved the false positive error rate in all of my models, as mentioned in the\n",
      "# above paragraph.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.77777778  0.79032258  0.80645161]\n",
        "0.791517323775\n",
        "Number of mislabeled points : 2\n",
        "Number of sad articles marked uplifting: : 2\n",
        "Number of uplifting articles marked sad: : 0\n"
       ]
      }
     ],
     "prompt_number": 354
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Unsuccessfully using RandomForest to find individual predictive words:\n",
      "\n",
      "# model = RandomForestClassifier(n_estimators=10)\n",
      "# trained_model = model.fit(X_train.toarray(), articles.uplifting)\n",
      "# print sorted(zip(model.feature_importances_, vectorizer.get_feature_names()), reverse=False)[:10]\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(-0.00026842446775705311, u'just'), (-0.00017957616157606354, u'heating'), (-0.00017434686158297237, u'father'), (-0.00016496030977932824, u'13'), (-0.00013060084478259184, u'payment'), (-0.00011682079188759902, u'people'), (-0.00011473197819954399, u'ensure'), (-0.0001142100301558382, u'chapel'), (-9.3773646400939189e-05, u'long'), (-9.2756141387571834e-05, u'dep\\xf3sito')]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}