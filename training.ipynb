{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "import sklearn.cross_validation as cv\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.naive_bayes import MultinomialNB \n",
      "import pandas as pd\n",
      "import numpy as np\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ...continued from data_prep.ipynb\n",
      "\n",
      "# 3c) What did you learn from the initial exploration phase?\n",
      "\n",
      "# To create an uplifting/joyful set of articles, do I:\n",
      "# 1) cherry-pick joyful articles?\n",
      "# 2) filter out the negative ones?\n",
      "\n",
      "# There are clear (universal, unambiguous) positives:\n",
      "# \"Freed Captive Is Reunited With His Family in Britain\"\n",
      "\n",
      "# Clear negatives:\n",
      "# \"Brazil: Deadly Mudslide at Resort Near Rio\"\n",
      "\n",
      "# And many, many ambiguous articles:\n",
      "# C.I.A. Takes On Expanded Role On Front Lines\n",
      "# Rush Limbaugh Released From Hospital\n",
      "\n",
      "# I decided to score an article as \"joyful\" only if it made me joyful.\n",
      "# That way, I maintained a clear signal inspite of the noise from negative\n",
      "# and ambiguous articles. I also accepted that controlling for individual bias\n",
      "# was outside the scope of this project. All articles were scored with my personal bias.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Note, as mentioned in my presentation and later in this notebook, I decided to add additional\n",
      "# joyful-scored articles to my corpus. The file below is the final training data set.\n",
      "\n",
      "articles = pd.read_csv('2010_ny_added_positives.csv')\n",
      "\n",
      "# articles = articles[:187]\n",
      "# I tried training models on subsets of my master set of ~769 articles. Predictably, the models \n",
      "# degraded roughly linearly with the reduction of the number of training documents. \n",
      "# I could have vastly improved my models by adding additional training data, but I did not have\n",
      "# time to score thousands or millions of articles."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles.groupby(articles.uplifting).count()\n",
      "\n",
      "# uplifting_prior_prob = 94 / 769\n",
      "# base_prior_prob = 675 / 769\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Unnamed: 0</th>\n",
        "      <th>lead_paragraph</th>\n",
        "      <th>headline</th>\n",
        "      <th>uplifting</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>uplifting</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 675</td>\n",
        "      <td> 675</td>\n",
        "      <td> 675</td>\n",
        "      <td> 675</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>  94</td>\n",
        "      <td>  94</td>\n",
        "      <td>  94</td>\n",
        "      <td>  94</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>2 rows \u00d7 4 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "           Unnamed: 0  lead_paragraph  headline  uplifting\n",
        "uplifting                                                 \n",
        "0                 675             675       675        675\n",
        "1                  94              94        94         94\n",
        "\n",
        "[2 rows x 4 columns]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I tried training the data set on the 'headline' text only, 'lead_paragraph' only, and 'headline'\n",
      "# + 'lead paragraph.' The two merged features did not perform significantly better than \n",
      "# 'lead_paragraph' alone, so I used that feature set only from then on:\n",
      "\n",
      "# Naive Bayes model, GAUSS, 3-fold cross validation:\n",
      "# (lead_paragraph only)\n",
      "# 0.908       0.89558233  0.89156627]\n",
      "\n",
      "# (headline only)\n",
      "# [ 0.732       0.78714859  0.74297189]\n",
      "\n",
      "# [headline + paragraph]\n",
      "#  0.908       0.89156627  0.8875502 ]\n",
      "\n",
      "# Code commented out, as it was abandoned:\n",
      "\n",
      "#MERGE lead_paragraph and headline data into a single column (no significant improvement)\n",
      "# join_col = pd.Series([])\n",
      "# for i in range(len(articles.lead_paragraph)):\n",
      "#     join_col[i] = articles.lead_paragraph[i] + \" \" + articles.headline[i]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 4) Describe what kinds of statistical methods you used, and perhaps others you considered but\n",
      "# did not use, and how you decided what to use.\n",
      "\n",
      "# For all models, I first prepared the data by converting each article into a bag-of-words\n",
      "# TFIDF vector. Justifications:\n",
      "# a) TFIDF vectorization is space efficient, because it handles sparse data sets efficiently.\n",
      "#   That is, it does not need to store data on the global set of words for every document,\n",
      "#   but rather only the words that actually appear in that document.\n",
      "# b) TF IDF is well respected in the information retrieval community for accurately modeling\n",
      "#   documents as bags of words. For example, it is used in the Lucene Project to calculate \n",
      "#   document similarity: https://lucene.apache.org/.\n",
      "# c) TFIDF normalizes a term's frequency with respect to its inter-document scarcity. This\n",
      "#   prevents a model overfitting highly-used, but low-value words.\n",
      "\n",
      "vectorizer = TfidfVectorizer( stop_words='english')\n",
      "X_train = vectorizer.fit_transform(articles.lead_paragraph)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# MODELS\n",
      "\n",
      "# Note: uncomment a model assignment line below to instantiate a model for the rest of the notebook.\n",
      "\n",
      "\n",
      "# model = RandomForestClassifier(n_estimators=30)\n",
      "# RandomForest yielded an error of .87, with a false positive rate of 2/94 = 2.1%\n",
      "# Thus, it was the most accurate model. However, it required 30 estimators to attain this\n",
      "# level of accuracy, so I don't plan to use it in a production implementation that needs to be\n",
      "# updated in anything close to real time.\n",
      "\n",
      "# I started with Random Forest as an exploratory step. I \n",
      "# hoped to find that certain words were highly indicative of joyfulness in a document. \n",
      "# I planned to boost these \"super predictive\" words to improve my model, like this:\n",
      "#   score(word) = term frequency * inv. doc. frequency * k\n",
      "#   where k = 1 for regular words, and k = 1 + e for super predictive words.\n",
      "# Unfortunately, I found that different training runs yielded wildly different \n",
      "# super predictive words. I concluded that my document set was too sparse, and my documents\n",
      "# too short, to reliably find words that predict joy in isolation. An example list\n",
      "# is available  a few paragraphs below. Two additional example lists are available in \n",
      "# prezi.pdf, slide 14.\n",
      "\n",
      "\n",
      "model = GaussianNB( ) # assumes normal distribution of terms\n",
      "# GaussianNB yielded an error of .870, with a false positive rate of 4/94 = 4.6%\n",
      "\n",
      "\n",
      "# model = MultinomialNB(alpha=.1) \n",
      "# MultinomialNB's false positive rate approached 100% if alpha was higher or lower than .1 (\n",
      "# I discovered this by trial and error. Given that even with the correct alpha it did not \n",
      "# outperform GaussianNB, I discarded it.\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TRAIN\n",
      "trained_model = model.fit(X_train.toarray(), articles.uplifting)\n",
      "cvs = cross_val_score(trained_model, X_train.toarray(), articles.uplifting)\n",
      "\n",
      "# Prints the mean error for the model\n",
      "print np.array(cvs).sum() / 3\n",
      "\n",
      "\n",
      "y_pred = trained_model.predict(X_train.toarray())\n",
      "print(\"Number of mislabeled points : %d\" % (articles.uplifting != y_pred).sum())\n",
      "print(\"Number of sad articles marked uplifting: : %d\" % (articles.uplifting < y_pred).sum())\n",
      "print(\"Number of uplifting articles marked sad: : %d\" % (articles.uplifting > y_pred).sum())\n",
      "\n",
      "# False positives are much more damaging to this project, since 1 depressing document can ruin\n",
      "# the experience of reading 9 proceeding uplifting documents. However, falsely marking an\n",
      "# uplifting document as depressing will not noticeably affect the reading experience, as\n",
      "# other uplifting documents may be substituted.\n",
      "\n",
      "# Fortunately, I decided to break the model error into false positives and false negatives.\n",
      "# I found that, given that my data was skewed heavily towards non-joyful documents, my models\n",
      "# could yield an error of ~.90 while scoring every single joyful document incorrectly!\n",
      "# I was able to mitigate this by adding more joyful documents to the training set (20 more joyful\n",
      "# documents, or a 27% increase in joyful documents). This drastically improved the false positive \n",
      "# error rate in all of my models, as mentioned in the above paragraph.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.869984192607\n",
        "Number of mislabeled points : 4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of sad articles marked uplifting: : 4\n",
        "Number of uplifting articles marked sad: : 0\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Unsuccessfully using RandomForest to find individual predictive words:\n",
      "\n",
      "# model = RandomForestClassifier(n_estimators=10)\n",
      "# trained_model = model.fit(X_train.toarray(), articles.uplifting)\n",
      "# print sorted(zip(model.feature_importances_, vectorizer.get_feature_names()), reverse=False)[:10]\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(-0.00026842446775705311, u'just'), (-0.00017957616157606354, u'heating'), (-0.00017434686158297237, u'father'), (-0.00016496030977932824, u'13'), (-0.00013060084478259184, u'payment'), (-0.00011682079188759902, u'people'), (-0.00011473197819954399, u'ensure'), (-0.0001142100301558382, u'chapel'), (-9.3773646400939189e-05, u'long'), (-9.2756141387571834e-05, u'dep\\xf3sito')]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 5) What business applications do your findings have?\n",
      "\n",
      "# Any person or business that wants to curate uplifting or joyful text from a larger corpus will be able to use\n",
      "# this model. Since it only uses features residing in document text, this model should be portable to document corpori\n",
      "# beyond those consisting of news articles. \n",
      "#\n",
      "# Additionally, I am confident that this model can be modified to predict phenomena besides joy. Excitement, for example."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 6) Describe the implementation plan in detail from the ingesting of data to how end users would access it.\n",
      "\n",
      "# There are many ways to incorporate this model into a full-fledged application. Here is an implementation that I plan\n",
      "# to use after this course has finished:\n",
      "\n",
      "# 1. Select data sources for training step and for \"streaming\" step. I only selected one source, the New York Times, for this\n",
      "#    project.\n",
      "# 2. Write adapters for each data source so that they transform the data into a common format. \n",
      "#    Example 1: write a parser that retrieves articles from the New York Times Article API and writes \"headline\" and \"lead_paragraph\" \n",
      "#    attributes into CSV format. \n",
      "#    Example 2: writer a parser that crawls article links on the Boston Globe homepage and writes the article\n",
      "#    headlines and article bodies to CSV format.\n",
      "# 3. Write a script that iterates over the CSV document and inserts each entry into a SQL table, for convenience.\n",
      "# 4. Score each document. Use a pool of human scorers to reduce bias, or accept a certain level of bias in the model.\n",
      "# 5. Using scikit-learn, train a Naive Bayes model on the training data.\n",
      "# 6. Create a web application that retrieves fresh data from the data sources on a regular basis. Select the joyful documents\n",
      "#    for presentation. Render the documents.\n",
      "# 7. (optional enhancement) Create personalized models from the original base model. This can be done adding a document to the \n",
      "#    base model once a user has \"upvoted\" it, or demonstrated fondness for the document in a passive way (e.g. by clicking\n",
      "#    on the document's permamlink and staying on the document's page for a certain amount of time.\n",
      "#\n",
      "#    The computing power and space required for maintaining personalized models for each user may not be feasible. In this case,\n",
      "#    a collaborative filter system - also trained on user intent - may be maintained in parallel with the Bayesian model. \n",
      "#    A document's score, and thus the likelihood of it reaching a certain user, would be computed as: \n",
      "#    bayes_score(document) * collaborative_filter(document,user)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Thanks for reading!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}