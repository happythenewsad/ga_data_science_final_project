{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Peter Kong - April 30, 2014\n",
      "# Data Science Final Project\n",
      "# \"The News Junkie's Dilemma\"\n",
      "\n",
      "# 1. Problem: Many people consume a lot of news to stay informed. Much, if not most, of this news negative, \n",
      "# dealing with violence and destruction (see prezi.pdf, slide 3). We need a way to stay informed without \n",
      "# bombarding ourselves with so much negativity.\n",
      "\n",
      "# 2. Hypothesis: The words in a news article can predict the article's \"joy score.\"\n",
      "\n",
      "# 3. Description of data set.\n",
      "# 3a. How did you decide what features to use in your analysis?\n",
      "\n",
      "# I established a primary source of data - the New York Times Article Search API \n",
      "# (http://developer.nytimes.com/docs/read/article_search_api_v2) - and let that API's features and limitations\n",
      "# inform my feature selection. I downloaded sample data via ./main.rb and studied the available features:\n",
      "\n",
      "# $ruby ./main.rb\n",
      "import pandas as pd\n",
      "articles = pd.read_csv('data/2010_ny_1000_clean.csv')\n",
      "articles.head(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>web_url</th>\n",
        "      <th>snippet</th>\n",
        "      <th>lead_paragraph</th>\n",
        "      <th>abstract</th>\n",
        "      <th>print_page</th>\n",
        "      <th>blog</th>\n",
        "      <th>source</th>\n",
        "      <th>multimedia</th>\n",
        "      <th>headline</th>\n",
        "      <th>keywords</th>\n",
        "      <th>pub_date</th>\n",
        "      <th>document_type</th>\n",
        "      <th>news_desk</th>\n",
        "      <th>section_name</th>\n",
        "      <th>subsection_name</th>\n",
        "      <th>byline</th>\n",
        "      <th>type_of_material</th>\n",
        "      <th>_id</th>\n",
        "      <th>word_count</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> http://query.nytimes.com/gst/fullpage.html?res...</td>\n",
        "      <td> The Olympic snowboarding hopeful Kevin Pearce ...</td>\n",
        "      <td> The Olympic snowboarding hopeful Kevin Pearce ...</td>\n",
        "      <td> Kevin Pear\u00f9</td>\n",
        "      <td>  8</td>\n",
        "      <td> []</td>\n",
        "      <td> The New York Times</td>\n",
        "      <td> []</td>\n",
        "      <td> Pearce Is Critically Injured On Halfpipe</td>\n",
        "      <td> [{\"name\"=&gt;\"persons\", \"value\"=&gt;\"PEARCE, KEVIN\"}...</td>\n",
        "      <td> 2010-01-01T00:00:00Z</td>\n",
        "      <td> article</td>\n",
        "      <td>                          Sports Desk</td>\n",
        "      <td>       Sports</td>\n",
        "      <td> NaN</td>\n",
        "      <td> {\"person\"=&gt;[{\"organization\"=&gt;\"\", \"role\"=&gt;\"repo...</td>\n",
        "      <td> News</td>\n",
        "      <td> 4fd2a6998eb7c8105d88f278</td>\n",
        "      <td> 281</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> http://query.nytimes.com/gst/fullpage.html?res...</td>\n",
        "      <td> 2:30 P.M. (PBS) FROM VIENNA: NEW YEAR'S CELEBR...</td>\n",
        "      <td> 2:30 P.M. (PBS) FROM VIENNA: NEW YEAR'S CELEBR...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td> 24</td>\n",
        "      <td> []</td>\n",
        "      <td> The New York Times</td>\n",
        "      <td> []</td>\n",
        "      <td>                          What's On Today</td>\n",
        "      <td>                                                []</td>\n",
        "      <td> 2010-01-01T00:00:00Z</td>\n",
        "      <td> article</td>\n",
        "      <td> Movies, Performing Arts/Weekend Desk</td>\n",
        "      <td> Movies; Arts</td>\n",
        "      <td> NaN</td>\n",
        "      <td> {\"person\"=&gt;[{\"organization\"=&gt;\"\", \"role\"=&gt;\"repo...</td>\n",
        "      <td> News</td>\n",
        "      <td> 4fd2a4b78eb7c8105d88b6c7</td>\n",
        "      <td> 599</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>2 rows \u00d7 19 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "                                             web_url  \\\n",
        "0  http://query.nytimes.com/gst/fullpage.html?res...   \n",
        "1  http://query.nytimes.com/gst/fullpage.html?res...   \n",
        "\n",
        "                                             snippet  \\\n",
        "0  The Olympic snowboarding hopeful Kevin Pearce ...   \n",
        "1  2:30 P.M. (PBS) FROM VIENNA: NEW YEAR'S CELEBR...   \n",
        "\n",
        "                                      lead_paragraph     abstract print_page  \\\n",
        "0  The Olympic snowboarding hopeful Kevin Pearce ...  Kevin Pear\u00f9          8   \n",
        "1  2:30 P.M. (PBS) FROM VIENNA: NEW YEAR'S CELEBR...          NaN         24   \n",
        "\n",
        "  blog              source multimedia  \\\n",
        "0   []  The New York Times         []   \n",
        "1   []  The New York Times         []   \n",
        "\n",
        "                                   headline  \\\n",
        "0  Pearce Is Critically Injured On Halfpipe   \n",
        "1                           What's On Today   \n",
        "\n",
        "                                            keywords              pub_date  \\\n",
        "0  [{\"name\"=>\"persons\", \"value\"=>\"PEARCE, KEVIN\"}...  2010-01-01T00:00:00Z   \n",
        "1                                                 []  2010-01-01T00:00:00Z   \n",
        "\n",
        "  document_type                             news_desk  section_name  \\\n",
        "0       article                           Sports Desk        Sports   \n",
        "1       article  Movies, Performing Arts/Weekend Desk  Movies; Arts   \n",
        "\n",
        "  subsection_name                                             byline  \\\n",
        "0             NaN  {\"person\"=>[{\"organization\"=>\"\", \"role\"=>\"repo...   \n",
        "1             NaN  {\"person\"=>[{\"organization\"=>\"\", \"role\"=>\"repo...   \n",
        "\n",
        "  type_of_material                       _id  word_count  \n",
        "0             News  4fd2a6998eb7c8105d88f278         281  \n",
        "1             News  4fd2a4b78eb7c8105d88b6c7         599  \n",
        "\n",
        "[2 rows x 19 columns]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Many features, like \"snippet,\" \"headline,\" and \"byline\" have similar data. Others, like \"web_url\" did not have a meaningful\n",
      "# relationship with the words in the article, so I removed those. I found that \"headline\" and \"lead_paragraph\" \n",
      "# were the most consistent and non-null text fields. They also held the most learnable information (i.e. words).\n",
      "# I decided to use \"headline\" and \"lead_paragraph\" features to score and train my model.\n",
      "\n",
      "# 3b. What challenges did you face in terms of obtaining and organizing the data?\n",
      "\n",
      "# The quality of the data was quite inconsistent. I initially tried using some old article data from the 1850's, but found\n",
      "# that a lot of articles head missing or truncated headlines. Other features, like snippet, were also inconsistently\n",
      "# represented. This was concerning since using this data would prevent me from adding additional features if I wanted to\n",
      "# in the future.\n",
      "\n",
      "pd.read_csv('data/old_data.csv').head(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>web_url</th>\n",
        "      <th>snippet</th>\n",
        "      <th>lead_paragraph</th>\n",
        "      <th>abstract</th>\n",
        "      <th>print_page</th>\n",
        "      <th>blog</th>\n",
        "      <th>source</th>\n",
        "      <th>multimedia</th>\n",
        "      <th>headline</th>\n",
        "      <th>keywords</th>\n",
        "      <th>pub_date</th>\n",
        "      <th>document_type</th>\n",
        "      <th>news_desk</th>\n",
        "      <th>section_name</th>\n",
        "      <th>subsection_name</th>\n",
        "      <th>byline</th>\n",
        "      <th>type_of_material</th>\n",
        "      <th>_id</th>\n",
        "      <th>word_count</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> http://query.nytimes.com/gst/abstract.html?res...</td>\n",
        "      <td> The Royal Mail steamship Canada, Captain HARRI...</td>\n",
        "      <td> The Royal Mail steamship Canada, Captain HARRI...</td>\n",
        "      <td> Progress of the Rebellion in</td>\n",
        "      <td> 2</td>\n",
        "      <td> []</td>\n",
        "      <td> The New York Times</td>\n",
        "      <td> []</td>\n",
        "      <td> {\"main\"=&gt;\"THREE DAYS LATER FROM EUROPE; ARRIVA...</td>\n",
        "      <td> [{\"name\"=&gt;\"glocations\", \"value\"=&gt;\"CHINA\"}]</td>\n",
        "      <td> 1851-10-01T00:03:58Z</td>\n",
        "      <td> article</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> Article</td>\n",
        "      <td> 4fbfd26a45c1498b0d005c2c</td>\n",
        "      <td> 2152</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> http://query.nytimes.com/gst/abstract.html?res...</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td>                          NaN</td>\n",
        "      <td> 4</td>\n",
        "      <td> []</td>\n",
        "      <td> The New York Times</td>\n",
        "      <td> []</td>\n",
        "      <td> {\"main\"=&gt;\"MARINE INTELLIGENCE.; Cleared. Arriv...</td>\n",
        "      <td>                                         []</td>\n",
        "      <td> 1851-10-01T00:03:58Z</td>\n",
        "      <td> article</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> Article</td>\n",
        "      <td> 4fbfd24945c1498b0d0050b1</td>\n",
        "      <td> 1281</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>2 rows \u00d7 19 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "                                             web_url  \\\n",
        "0  http://query.nytimes.com/gst/abstract.html?res...   \n",
        "1  http://query.nytimes.com/gst/abstract.html?res...   \n",
        "\n",
        "                                             snippet  \\\n",
        "0  The Royal Mail steamship Canada, Captain HARRI...   \n",
        "1                                                NaN   \n",
        "\n",
        "                                      lead_paragraph  \\\n",
        "0  The Royal Mail steamship Canada, Captain HARRI...   \n",
        "1                                                NaN   \n",
        "\n",
        "                       abstract  print_page blog              source  \\\n",
        "0  Progress of the Rebellion in           2   []  The New York Times   \n",
        "1                           NaN           4   []  The New York Times   \n",
        "\n",
        "  multimedia                                           headline  \\\n",
        "0         []  {\"main\"=>\"THREE DAYS LATER FROM EUROPE; ARRIVA...   \n",
        "1         []  {\"main\"=>\"MARINE INTELLIGENCE.; Cleared. Arriv...   \n",
        "\n",
        "                                     keywords              pub_date  \\\n",
        "0  [{\"name\"=>\"glocations\", \"value\"=>\"CHINA\"}]  1851-10-01T00:03:58Z   \n",
        "1                                          []  1851-10-01T00:03:58Z   \n",
        "\n",
        "  document_type  news_desk  section_name  subsection_name byline  \\\n",
        "0       article        NaN           NaN              NaN    NaN   \n",
        "1       article        NaN           NaN              NaN    NaN   \n",
        "\n",
        "  type_of_material                       _id  word_count  \n",
        "0          Article  4fbfd26a45c1498b0d005c2c        2152  \n",
        "1          Article  4fbfd24945c1498b0d0050b1        1281  \n",
        "\n",
        "[2 rows x 19 columns]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I chose to limit my article retrieval to articles published between Jan 1, 2010 and Jan 1, 2012. Even so, data cleaning \n",
      "# was essential: \n",
      "\n",
      "deduped = articles.drop_duplicates(cols='headline')\n",
      "deduped.count() #167 duplicates!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "web_url             833\n",
        "snippet             829\n",
        "lead_paragraph      827\n",
        "abstract            461\n",
        "print_page          591\n",
        "blog                833\n",
        "source              790\n",
        "multimedia          833\n",
        "headline            833\n",
        "keywords            833\n",
        "pub_date            833\n",
        "document_type       833\n",
        "news_desk           680\n",
        "section_name        779\n",
        "subsection_name       2\n",
        "byline              564\n",
        "type_of_material    790\n",
        "_id                 833\n",
        "word_count          830\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_columns = deduped[['lead_paragraph','headline']]\n",
      "text_columns = text_columns.dropna()\n",
      "print(text_columns.count()) #6 null headlines"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "lead_paragraph    827\n",
        "headline          827\n",
        "dtype: int64\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# It was obvious that many headlines were too short to be predictive of \"joy.\" I removed all articles\n",
      "# with headlines shorter than 20 characters.\n",
      "\n",
      "headlines = text_columns.headline\n",
      "ctr = 0\n",
      "for counter in range(len(headlines)):\n",
      "    try:\n",
      "        if len(headlines[counter]) < 20:\n",
      "            ctr += 1\n",
      "            headlines[counter] = None\n",
      "    except Exception, e:\n",
      "        1\n",
      "\n",
      "print \"you nulled this many short headlines:\"\n",
      "print ctr\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "you nulled this many short headlines:\n",
        "79\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "culled = text_columns.dropna()\n",
      "culled.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "lead_paragraph    748\n",
        "headline          748\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I added a scoring column to the cleaned dataset.\n",
      "\n",
      "culled['uplifting'] = 0\n",
      "culled.to_csv('data/2010_ny_1000_culled.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    }
   ],
   "metadata": {}
  }
 ]
}